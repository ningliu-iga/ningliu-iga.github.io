---
---

@string{aps = {American Physical Society,}}


@article{liu2024disentangled,
  title={Disentangled Representation Learning for Parametric Partial Differential Equations},
  author={<u>Liu, Ning</u> and Zhang, Lu and Gao, Tian and Yu, Yue},
  journal={arXiv preprint arXiv:2410.02136},
  year={2024},
  pdf={https://arxiv.org/abs/2410.02136},
  code={https://github.com/hanjq17/SGNN},
  abstract = {Neural operators (NOs) have demonstrated remarkable success in learning mappings between function spaces, serving as efficient approximators for the forward solutions of complex physical systems governed by partial differential equations (PDEs). However, while effective as black-box solvers, they offer limited insight into the underlying physical mechanism, due to the lack of interpretable representations of the physical parameters that drive the system. To tackle this challenge, we propose a new paradigm for learning disentangled representations from neural operator parameters, thereby effectively solving an inverse problem. Specifically, we introduce DisentangO, a novel hyper-neural operator architecture designed to unveil and disentangle the latent physical factors of variation embedded within the black-box neural operator parameters. At the core of DisentangO is a multi-task neural operator architecture that distills the varying parameters of the governing PDE through a task-wise adaptive layer, coupled with a hierarchical variational autoencoder that disentangles these variations into identifiable latent factors. By learning these disentangled representations, our model not only enhances physical interpretability but also enables more robust generalization across diverse physical systems. Empirical evaluations across supervised, semi-supervised, and unsupervised learning contexts show that DisentangO effectively extracts meaningful and interpretable latent features, bridging the divide between predictive performance and physical understanding in neural operator frameworks.},
  selected={true},
  preview={DisentangO.png}
}

@inproceedings{yu2024nonlocal,
abbr={NeurIPS},
award={Spotlight},
honor={Spotlight paper},
title={Nonlocal attention operator: Materializing hidden knowledge towards interpretable physics discovery},
author={Yu, Yue and </u>Liu, Ning</u> and Lu, Fei and Gao, Tian and Jafarzadeh, Siavash and Silling, Stewart},
booktitle={Thirty-Eighth Conference on Neural Information Processing Systems},
year={2024},
pdf={https://arxiv.org/abs/2408.07307},
code={https://github.com/fishmoon1234/NAO},
abstract = {Despite the recent popularity of attention-based neural architectures in core AI fields like natural language processing (NLP) and computer vision (CV), their potential in modeling complex physical systems remains under-explored. Learning problems in physical systems are often characterized as discovering operators that map between function spaces based on a few instances of function pairs. This task frequently presents a severely ill-posed PDE inverse problem. In this work, we propose a novel neural operator architecture based on the attention mechanism, which we coin Nonlocal Attention Operator (NAO), and explore its capability towards developing a foundation physical model. In particular, we show that the attention mechanism is equivalent to a double integral operator that enables nonlocal interactions among spatial tokens, with a data-dependent kernel characterizing the inverse mapping from data to the hidden parameter field of the underlying operator. As such, the attention mechanism extracts global prior information from training data generated by multiple systems, and suggests the exploratory space in the form of a nonlinear kernel map. Consequently, NAO can address ill-posedness and rank deficiency in inverse PDE problems by encoding regularization and achieving generalizability. We empirically demonstrate the advantages of NAO over baseline neural models in terms of generalizability to unseen data resolutions and system states. Our work not only suggests a novel neural operator architecture for learning interpretable foundation models of physical systems, but also offers a new perspective towards understanding the attention mechanism.},
selected={true},
preview={NAO.png}
}

@article{liu2024large,
  title={Large language models, physics-based modeling, experimental measurements: the trinity of data-scarce learning of polymer properties},
  author={</u>Liu, Ning</u> and Jafarzadeh, Siavash and Lattimer, Brian Y and Ni, Shuna and Lua, Jim and Yu, Yue},
  journal={arXiv preprint arXiv:2407.02770},
  year={2024},
  pdf={https://arxiv.org/abs/2407.02770},
  code={https://github.com/ningliu-iga/TrinityLLM},
  abstract = {Large language models (LLMs) bear promise as a fast and accurate material modeling paradigm for evaluation, analysis, and design. Their vast number of trainable parameters necessitates a wealth of data to achieve accuracy and mitigate overfitting. However, experimental measurements are often limited and costly to obtain in sufficient quantities for finetuning. To this end, we present a physics-based training pipeline that tackles the pathology of data scarcity. The core enabler is a physics-based modeling framework that generates a multitude of synthetic data to align the LLM to a physically consistent initial state before finetuning. Our framework features a two-phase training strategy: (1) utilizing the large-in-amount while less accurate synthetic data for supervised pretraining, and (2) finetuning the phase-1 model with limited experimental data. We empirically demonstrate that supervised pretraining is vital to obtaining accurate finetuned LLMs, via the lens of learning polymer flammability metrics where cone calorimeter data is sparse.},
  selected={true},
  preview={trinity.png}
}

